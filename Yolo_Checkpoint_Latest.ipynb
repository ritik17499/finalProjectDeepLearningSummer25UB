{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/MPI/gcc/11.2.0/openmpi/4.1.1/pytorch/1.13.1-CUDA-11.8.0/lib/python3.9/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/Compiler/gcccore/11.2.0/python/3.9.6/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is enabled. GPU is available.\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "Device name: Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is enabled. GPU is available.\")\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"CUDA is not enabled. GPU is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchvision==0.14.1\n",
      "  Downloading torchvision-0.14.1-cp39-cp39-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 3.0 MB/s eta 0:00:01��████    | 21.1 MB 3.0 MB/s eta 0:00:02\n",
      "\u001b[?25hInstalling collected packages: torchvision\n",
      "Successfully installed torchvision-0.14.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.2; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/Compiler/gcccore/11.2.0/python/3.9.6/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install torchvision==0.14.1 --no-deps --no-cache-dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy==1.24.4\n",
      "  Downloading numpy-1.24.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3 MB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 16] Device or resource busy: '.nfs3ee5a9eaf8dec5eb000010b0'\n",
      "\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.2; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/Compiler/gcccore/11.2.0/python/3.9.6/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy==1.24.4 --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting Pillow\n",
      "  Downloading pillow-11.2.1-cp39-cp39-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.6 MB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: Pillow\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.14.1 requires torch==1.13.1, which is not installed.\u001b[0m\n",
      "Successfully installed Pillow-11.2.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.2; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/cvmfs/soft.ccr.buffalo.edu/versions/2023.01/easybuild/software/avx512/Compiler/gcccore/11.2.0/python/3.9.6/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/hgowdarm/.local/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "\n",
    "# Data Preprocessing (convert labels to YOLOv5 format)\n",
    "\n",
    "CLASS_MAP = {\n",
    "    'car': 0,\n",
    "    'bus': 1,\n",
    "    'truck': 2,\n",
    "    'person': 3,\n",
    "    'bike': 4,\n",
    "    'motor': 5,\n",
    "    'traffic light': 6,\n",
    "    'traffic sign': 7,\n",
    "    'train': 8,\n",
    "    'rider': 9\n",
    "}\n",
    "\n",
    "def convert_box_yolov5(box2d, w, h):\n",
    "    x1, y1, x2, y2 = box2d['x1'], box2d['y1'], box2d['x2'], box2d['y2']\n",
    "    x_center = (x1 + x2) / 2.0 / w\n",
    "    y_center = (y1 + y2) / 2.0 / h\n",
    "    bw = (x2 - x1) / w\n",
    "    bh = (y2 - y1) / h\n",
    "    return x_center, y_center, bw, bh\n",
    "\n",
    "def convert_annotations(label_dir, image_dir, output_img_dir, output_label_dir):\n",
    "    os.makedirs(output_img_dir, exist_ok=True)\n",
    "    os.makedirs(output_label_dir, exist_ok=True)\n",
    "    converted = 0\n",
    "    for img_name in os.listdir(image_dir):\n",
    "        if not img_name.endswith('.jpg'):\n",
    "            continue\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        json_path = os.path.join(label_dir, img_name.replace('.jpg', '.json'))\n",
    "        if not os.path.exists(json_path):\n",
    "            continue\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        frame = data.get('frames', [{}])[0]\n",
    "        objects = frame.get('objects', [])\n",
    "        lines = []\n",
    "        image = Image.open(img_path)\n",
    "        w, h = image.size\n",
    "        for obj in objects:\n",
    "            if 'box2d' not in obj:\n",
    "                continue\n",
    "            cls = obj['category']\n",
    "            if cls not in CLASS_MAP:\n",
    "                continue\n",
    "            x, y, bw, bh = convert_box_yolov5(obj['box2d'], w, h)\n",
    "            lines.append(f\"{CLASS_MAP[cls]} {x:.6f} {y:.6f} {bw:.6f} {bh:.6f}\")\n",
    "        if lines:\n",
    "            label_file = os.path.join(output_label_dir, img_name.replace('.jpg', '.txt'))\n",
    "            with open(label_file, 'w') as f:\n",
    "                f.write('\\n'.join(lines))\n",
    "            shutil.copy(img_path, os.path.join(output_img_dir, img_name))\n",
    "            converted += 1\n",
    "    print(f\" Converted {converted} images to YOLOv5 format\")\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, device='cuda' if torch.cuda.is_available() else 'cpu', max_batches=3):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    print(\"\\n Running evaluation on a few batches...\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (imgs, targets) in enumerate(dataloader):\n",
    "            if batch_idx >= max_batches:\n",
    "                break\n",
    "            imgs = imgs.to(device)\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            print(f\"\\nBatch {batch_idx + 1}\")\n",
    "            print(f\" - Input shape: {imgs.shape}\")\n",
    "            print(f\" - Output shape: {outputs.shape}\")\n",
    "\n",
    "            b, ch, h, w = outputs.shape\n",
    "            anchors = 3\n",
    "            num_classes = int(ch / anchors - 5)\n",
    "            outputs = outputs.view(b, anchors, 5 + num_classes, h, w).permute(0, 1, 3, 4, 2)\n",
    "\n",
    "            conf = torch.sigmoid(outputs[..., 4])\n",
    "            class_scores = torch.sigmoid(outputs[..., 5:])\n",
    "\n",
    "            print(f\" - Objectness score (min/max): {conf.min().item():.4f} / {conf.max().item():.4f}\")\n",
    "            print(f\" - Class scores (min/max): {class_scores.min().item():.4f} / {class_scores.max().item():.4f}\")\n",
    "    print(\"\\n Evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Model Definition (YOLOv5-like)\n",
    "\n",
    "def conv_bn(in_channels, out_channels, kernel_size, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=kernel_size // 2, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.LeakyReLU(0.1)\n",
    "    )\n",
    "\n",
    "class CSPBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CSPBlock, self).__init__()\n",
    "        hidden = out_channels // 2\n",
    "        self.conv1 = conv_bn(in_channels, hidden, 1, 1)\n",
    "        self.conv2 = conv_bn(hidden, hidden, 3, 1)\n",
    "        self.conv3 = conv_bn(hidden * 2, out_channels, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.conv2(self.conv1(x))\n",
    "        y2 = self.conv1(x)\n",
    "        return self.conv3(torch.cat([y1, y2], dim=1))\n",
    "\n",
    "class SPPF(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, pool_size=5):\n",
    "        super(SPPF, self).__init__()\n",
    "        hidden = in_channels // 2\n",
    "        self.conv1 = conv_bn(in_channels, hidden, 1, 1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_size, stride=1, padding=pool_size // 2)\n",
    "        self.conv2 = conv_bn(hidden * 4, out_channels, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        y1 = self.pool(x)\n",
    "        y2 = self.pool(y1)\n",
    "        y3 = self.pool(y2)\n",
    "        return self.conv2(torch.cat([x, y1, y2, y3], 1))\n",
    "\n",
    "class YOLOv5Custom(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(YOLOv5Custom, self).__init__()\n",
    "        self.layer1 = conv_bn(3, 32, 3, 1)\n",
    "        self.layer2 = conv_bn(32, 64, 3, 2)\n",
    "        self.csp1 = CSPBlock(64, 64)\n",
    "        self.layer3 = conv_bn(64, 128, 3, 2)\n",
    "        self.csp2 = CSPBlock(128, 128)\n",
    "        self.layer4 = conv_bn(128, 256, 3, 2)\n",
    "        self.csp3 = CSPBlock(256, 256)\n",
    "        self.sppf = SPPF(256, 256)\n",
    "        self.head = nn.Sequential(\n",
    "            conv_bn(256, 128, 3, 1),\n",
    "            nn.Conv2d(128, 3 * (5 + num_classes), 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.csp1(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.csp2(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.csp3(x)\n",
    "        x = self.sppf(x)\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loss Function \n",
    "\n",
    "class YOLOLoss(nn.Module):\n",
    "    def __init__(self, lambda_coord=5.0, lambda_noobj=0.5):\n",
    "        super(YOLOLoss, self).__init__()\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        return torch.mean(predictions ** 2)\n",
    "\n",
    "\n",
    "# Dataset + Collate\n",
    "\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None, limit=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
    "        if limit:\n",
    "            self.image_files = self.image_files[:limit]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_file)\n",
    "        label_path = os.path.join(self.label_dir, img_file.replace('.jpg', '.txt'))\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        boxes = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f.readlines():\n",
    "                    cls, x, y, bw, bh = map(float, line.strip().split())\n",
    "                    boxes.append([cls, x, y, bw, bh])\n",
    "\n",
    "        return image, torch.tensor(boxes)\n",
    "\n",
    "def yolo_collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    images = torch.stack(images, dim=0)\n",
    "    return images, targets\n",
    "\n",
    "\n",
    "# Training Utilities\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for imgs, targets in dataloader:\n",
    "        imgs = imgs.to(device)\n",
    "        preds = model(imgs)\n",
    "        loss = loss_fn(preds, preds)  # placeholder\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def run_training(model, train_loader, val_loader, num_epochs=5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    loss_fn = YOLOLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "        print(f\"[Epoch {epoch+1}] Loss: {loss:.4f}\")\n",
    "    torch.save(model.state_dict(), \"yolov5_customm_checkpoint.pt\")\n",
    "    print(\" Weights saved to yolov5_customm_checkpoint.pt\")\n",
    "\n",
    "# Run Label Conversion Once\n",
    "if not os.path.exists(\"custom_yolo_dataset/labels/train\"):\n",
    "    convert_annotations(\"bdd100k/100k/train\", \"bdd100k/100k/train\",\n",
    "                        \"custom_yolo_dataset/images/train\", \"custom_yolo_dataset/labels/train\")\n",
    "\n",
    "if not os.path.exists(\"custom_yolo_dataset/labels/val\"):\n",
    "    convert_annotations(\"bdd100k/100k/val\", \"bdd100k/100k/val\",\n",
    "                        \"custom_yolo_dataset/images/val\", \"custom_yolo_dataset/labels/val\")\n",
    "# Dataset Setup\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = YOLODataset(\"custom_yolo_dataset/images/train\", \"custom_yolo_dataset/labels/train\", transform, limit=20000)\n",
    "val_full = YOLODataset(\"custom_yolo_dataset/images/val\", \"custom_yolo_dataset/labels/val\", transform)\n",
    "val_size = int(0.8 * len(val_full))\n",
    "test_size = len(val_full) - val_size\n",
    "val_dataset, test_dataset = random_split(val_full, [val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=yolo_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=yolo_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=yolo_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss: 0.0004\n",
      "[Epoch 2] Loss: 0.0000\n",
      "[Epoch 3] Loss: 0.0000\n",
      "[Epoch 4] Loss: 0.0000\n",
      "[Epoch 5] Loss: 0.0000\n",
      " Weights saved to yolov5_customm_checkpoint.pt\n",
      "\n",
      " Running evaluation on a few batches...\n",
      "\n",
      "Batch 1\n",
      " - Input shape: torch.Size([4, 3, 256, 256])\n",
      " - Output shape: torch.Size([4, 45, 32, 32])\n",
      " - Objectness score (min/max): 0.4996 / 0.5005\n",
      " - Class scores (min/max): 0.4994 / 0.5006\n",
      "\n",
      "Batch 2\n",
      " - Input shape: torch.Size([4, 3, 256, 256])\n",
      " - Output shape: torch.Size([4, 45, 32, 32])\n",
      " - Objectness score (min/max): 0.4997 / 0.5005\n",
      " - Class scores (min/max): 0.4995 / 0.5005\n",
      "\n",
      "Batch 3\n",
      " - Input shape: torch.Size([4, 3, 256, 256])\n",
      " - Output shape: torch.Size([4, 45, 32, 32])\n",
      " - Objectness score (min/max): 0.4999 / 0.5002\n",
      " - Class scores (min/max): 0.4996 / 0.5005\n",
      "\n",
      " Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run Training + Evaluation\n",
    "\n",
    "model = YOLOv5Custom(num_classes=10)\n",
    "run_training(model, train_loader, val_loader, num_epochs=5)\n",
    "\n",
    "# Evaluate model on test data\n",
    "evaluate_model(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
